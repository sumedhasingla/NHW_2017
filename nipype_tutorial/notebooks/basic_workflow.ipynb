{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "Although it would be possible to write analysis scripts using just Nipype [Interfaces](basic_interfaces.ipynb), and this may provide some advantages over directly making command-line calls, the main benefits of Nipype will come by creating workflows.\n",
    "\n",
    "A workflow controls the setup and the execution of individual interfaces. Let's assume you want to run multiple interfaces in a specific order, where some have to wait for others to finish while others can be executed in parallel. The nice thing about a nipype workflow is, that the workflow  will take care of input and output of each interface and arrange the execution of each interface in the most efficient way.\n",
    "\n",
    "A workflow therefore consists of multiple [Nodes](basic_nodes.ipynb), each representing a specific [Interface](basic_interfaces.ipynb) and directed connection between those nodes. Those connections specify which output of which node should be used as an input for another node. To better understand why this is so great, let's look at an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before we can start, let's first load some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import nibabel as nb\n",
    "\n",
    "# Let's create a short helper function to plot 3D NIfTI images\n",
    "def plot_slice(fname):\n",
    "\n",
    "    # Load the image\n",
    "    img = nb.load(fname)\n",
    "    data = img.get_data()\n",
    "\n",
    "    # Cut in the middle of the brain\n",
    "    cut = int(data.shape[-1]/2) + 10\n",
    "\n",
    "    # Plot the data\n",
    "    imshow(np.rot90(data[..., cut]), cmap=\"gray\")\n",
    "    gca().set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - ``Command-line`` execution\n",
    "\n",
    "Let's take a look at a small preprocessing analysis where we would like to perform the following steps of processing:\n",
    "\n",
    "    - Skullstrip an image to obtain a mask\n",
    "    - Smooth the original image\n",
    "    - Mask the smoothed image\n",
    "\n",
    "This could all very well be done with the following shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ANAT_NAME=sub-02_ses-test_T1w\n",
    "ANAT=/data/ds000114/sub-02/ses-test/anat/${ANAT_NAME}\n",
    "bet ${ANAT} /output/${ANAT_NAME}_brain -m -f 0.3\n",
    "fslmaths ${ANAT} -s 2 /output/${ANAT_NAME}_smooth\n",
    "fslmaths /output/${ANAT_NAME}_smooth -mas /output/${ANAT_NAME}_brain_mask /output/${ANAT_NAME}_smooth_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simple and straightforward. We can see that this does exactly what we wanted by plotting the four steps of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 4))\n",
    "for i, img in enumerate([\"T1w\", \"T1w_smooth\",\n",
    "                         \"T1w_brain_mask\", \"T1w_smooth_mask\"]):\n",
    "    f.add_subplot(1, 4, i + 1)\n",
    "    if i == 0:\n",
    "        plot_slice(\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_%s.nii.gz\" % img)\n",
    "    else:\n",
    "        plot_slice(\"/output/sub-02_ses-test_%s.nii.gz\" % img)\n",
    "    plt.title(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - ``Interface`` execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what this would look like if we used Nipype, but only the Interfaces functionality. It's simple enough to write a basic procedural script, this time in Python, to do the same thing as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces import fsl\n",
    "\n",
    "# Skullstrip process\n",
    "skullstrip = fsl.BET(\n",
    "    in_file=\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\",\n",
    "    out_file=\"/output/sub-02_T1w_brain.nii.gz\",\n",
    "    mask=True)\n",
    "skullstrip.run()\n",
    "\n",
    "# Smoothing process\n",
    "smooth = fsl.IsotropicSmooth(\n",
    "    in_file=\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\",\n",
    "    out_file=\"/output/sub-02_T1w_smooth.nii.gz\",\n",
    "    fwhm=4)\n",
    "smooth.run()\n",
    "\n",
    "# Masking process\n",
    "mask = fsl.ApplyMask(\n",
    "    in_file=\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\",\n",
    "    out_file=\"/output/sub-02_T1w_smooth_mask.nii.gz\",\n",
    "    mask_file=\"/output/sub-02_T1w_brain_mask.nii.gz\")\n",
    "mask.run()\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "for i, img in enumerate([\"T1w\", \"T1w_smooth\",\n",
    "                         \"T1w_brain_mask\", \"T1w_smooth_mask\"]):\n",
    "    f.add_subplot(1, 4, i + 1)\n",
    "    if i == 0:\n",
    "        plot_slice(\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_%s.nii.gz\" % img)\n",
    "    else:\n",
    "        plot_slice(\"/output/sub-02_%s.nii.gz\" % img)\n",
    "    plt.title(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more verbose, although it does have its advantages. There's the automated input validation we saw previously, some of the options are named more meaningfully, and you don't need to remember, for example, that fslmaths' smoothing kernel is set in sigma instead of FWHM -- Nipype does that conversion behind the scenes.\n",
    "\n",
    "### Can't we optimize that a bit?\n",
    "\n",
    "As we can see above, the inputs for the **``mask``** routine ``in_file`` and ``mask_file`` are actually the output of **``skullstrip``** and **``smooth``**. We therefore somehow want to connect them. This can be accomplisehd by saving the executed routines under a given object and than using the output of those objects as input for other routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces import fsl\n",
    "\n",
    "# Skullstrip process\n",
    "skullstrip = fsl.BET(\n",
    "    in_file=\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\", mask=True)\n",
    "bet_result = skullstrip.run()  # skullstrip object\n",
    "\n",
    "# Smooth process\n",
    "smooth = fsl.IsotropicSmooth(\n",
    "    in_file=\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\", fwhm=4)\n",
    "smooth_result = smooth.run()  # smooth object\n",
    "\n",
    "# Mask process\n",
    "mask = fsl.ApplyMask(in_file=smooth_result.outputs.out_file,\n",
    "                     mask_file=bet_result.outputs.mask_file)\n",
    "mask_result = mask.run()\n",
    "\n",
    "f = plt.figure(figsize=(12, 4))\n",
    "for i, img in enumerate([skullstrip.inputs.in_file, smooth_result.outputs.out_file,\n",
    "                         bet_result.outputs.mask_file, mask_result.outputs.out_file]):\n",
    "    f.add_subplot(1, 4, i + 1)\n",
    "    plot_slice(img)\n",
    "    plt.title(img.split('/')[-1].split('.')[0].split('test_')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we didn't need to name the intermediate files; Nipype did that behind the scenes, and then we passed the result object (which knows those names) onto the next step in the processing stream. This is somewhat more concise than the example above, but it's still a procedural script. And the dependency relationship between the stages of processing is not particularly obvious. To address these issues, and to provide solutions to problems we might not know we have yet, Nipype offers **Workflows.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3 - ``Workflow`` execution\n",
    "\n",
    "What we've implicitly done above is to encode our processing stream as a directed acyclic graphs: each stage of processing is a node in this graph, and some nodes are unidirectionally dependent on others. In this case there is one input file and several output files, but there are no cycles -- there's a clear line of directionality to the processing. What the Node and Workflow classes do is make these relationships more explicit.\n",
    "\n",
    "The basic architecture is that the Node provides a light wrapper around an Interface. It exposes the inputs and outputs of the Interface as its own, but it adds some additional functionality that allows you to connect Nodes into a Workflow.\n",
    "\n",
    "Let's rewrite the above script with these tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Node and Workflow object and FSL interface\n",
    "from nipype import Node, Workflow\n",
    "from nipype.interfaces import fsl\n",
    "\n",
    "# For reasons that will later become clear, it's important to\n",
    "# pass filenames to Nodes as absolute paths\n",
    "from os.path import abspath\n",
    "in_file = abspath(\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\")\n",
    "\n",
    "# Skullstrip process\n",
    "skullstrip = Node(fsl.BET(in_file=in_file, mask=True), name=\"skullstrip\")\n",
    "\n",
    "# Smooth process\n",
    "smooth = Node(fsl.IsotropicSmooth(in_file=in_file, fwhm=4), name=\"smooth\")\n",
    "\n",
    "# Mask process\n",
    "mask = Node(fsl.ApplyMask(), name=\"mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks mostly similar to what we did above, but we've left out the two crucial inputs to the ApplyMask step. We'll set those up by defining a Workflow object and then making *connections* among the Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiation of a workflow\n",
    "wf = Workflow(name=\"smoothflow\", base_dir=\"/output/working_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Workflow object has a method called ``connect`` that is going to do most of the work here. This routine also checks if inputs and outputs are actually provided by the nodes that are being connected.\n",
    "\n",
    "There are two different ways to call ``connect``:\n",
    "\n",
    "    connect(source, \"source_output\", dest, \"dest_input\")\n",
    "\n",
    "    connect([(source, dest, [(\"source_output1\", \"dest_input1\"),\n",
    "                             (\"source_output2\", \"dest_input2\")\n",
    "                             ])\n",
    "             ])\n",
    "\n",
    "With the first approach you can establish one connection at a time. With the second you can establish multiple connects between two nodes at once. In either case, you're providing it with four pieces of information to define the connection:\n",
    "\n",
    "- The source node object\n",
    "- The name of the output field from the source node\n",
    "- The destination node object\n",
    "- The name of the input field from the destination node\n",
    "\n",
    "We'll illustrate each method in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the \"simple\", but more restricted method\n",
    "wf.connect(skullstrip, \"mask_file\", mask, \"mask_file\")\n",
    "\n",
    "# Now the more complicated method\n",
    "wf.connect([(smooth, mask, [(\"out_file\", \"in_file\")])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the workflow is complete!\n",
    "\n",
    "Above, we mentioned that the workflow can be thought of as a directed acyclic graph. In fact, that's literally how it's represented behind the scenes, and we can use that to explore the workflow visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.write_graph(\"workflow_graph.dot\")\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/output/working_dir/smoothflow/workflow_graph.dot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation makes the dependency structure of the workflow obvious. (By the way, the names of the nodes in this graph are the names we gave our Node objects above, so pick something meaningful for those!)\n",
    "\n",
    "Certain graph types also allow you to further inspect the individual connections between the nodes. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.write_graph(graph2use='flat')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/output/working_dir/smoothflow/graph_detailed.dot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see very clearly, that the output ``mask_file`` of the ``skullstrip`` node is used as the input ``mask_file`` of the ``mask`` node. For more information on graph visualization, see the [Graph Visualization](./basic_graph_visualization.ipynb) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's come back to our example. At this point, all we've done is define the workflow. We haven't executed any code yet. Much like Interface objects, the Workflow object has a ``run`` method that we can call so that it executes. Let's do that and then examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base directory for the working directory\n",
    "wf.base_dir = \"/output/working_dir\"\n",
    "\n",
    "# Execute the workflow\n",
    "wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The specification of ``base_dir`` is very important (and is why we needed to use absolute paths above), because otherwise all the outputs would be saved somewhere in the temporary files.** Unlike interfaces, which by default spit out results to the local directry, the Workflow engine executes things off in its own directory hierarchy.\n",
    "\n",
    "Let's take a look at the resulting images to convince ourselves we've done the same thing as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 4))\n",
    "for i, img in enumerate([\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\",\n",
    "                         \"/output/working_dir/smoothflow/smooth/sub-02_ses-test_T1w_smooth.nii.gz\",\n",
    "                         \"/output/working_dir/smoothflow/skullstrip/sub-02_ses-test_T1w_brain_mask.nii.gz\",\n",
    "                         \"/output/working_dir/smoothflow/mask/sub-02_ses-test_T1w_smooth_masked.nii.gz\"]):\n",
    "    f.add_subplot(1, 4, i + 1)\n",
    "    plot_slice(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfet!\n",
    "\n",
    "Let's also have a closer look at the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree /output/working_dir/smoothflow/ -I '*js|*json|*html|*pklz|_report'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the name of the working directory is the name we gave the workflow ``base_dir``. And the name of the folder within is the name of the workflow object ``smoothflow``. Each node of the workflow has its' own subfolder in the ``smoothflow`` folder. And each of those subfolders contains the output of the node as well as some additional files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A workflow inside a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start writing full-fledged analysis workflows, things can get quite complicated. Some aspects of neuroimaging analysis can be thought of as a coherent step at a level more abstract than the execution of a single command line binary. For instance, in the standard FEAT script in FSL, several calls are made in the process of using `susan` to perform nonlinear smoothing on an image. In Nipype, you can write **nested workflows**, where a sub-workflow can take the place of a Node in a given script.\n",
    "\n",
    "Let's use the prepackaged `susan` workflow that ships with Nipype to replace our Gaussian filtering node and demonstrate how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.workflows.fmri.fsl import create_susan_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling this function will return a pre-written `Workflow` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "susan = create_susan_smooth(separate_masks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the graph to see what happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "susan.write_graph(\"susan_workflow.dot\")\n",
    "from IPython.display import Image\n",
    "Image(filename=\"susan_workflow.dot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the workflow has an `inputnode` and an `outputnode`. While not strictly necessary, this is standard practice for workflows (especially those that are intended to be used as nested workflows in the context of a longer analysis graph) and makes it more clear how to connect inputs and outputs from this workflow.\n",
    "\n",
    "Let's take a look at what those inputs and outputs are. Like Nodes, Workflows have `inputs` and `outputs` attributes that take a second sub-attribute corresponding to the specific node we want to make connections to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inputs:\\n\", susan.inputs.inputnode)\n",
    "print(\"Outputs:\\n\", susan.outputs.outputnode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `inputnode` and `outputnode` are just conventions, and the Workflow object exposes connections to all of its component nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "susan.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we would write a new workflow that uses this nested smoothing step.\n",
    "\n",
    "The susan workflow actually expects to receive and output a list of files (it's intended to be executed on each of several runs of fMRI data). We'll cover exactly how that works in later tutorials, but for the moment we need to add an additional ``Function`` node to deal with the fact that ``susan`` is outputting a list. We can use a simple `lambda` function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype import Function\n",
    "extract_func = lambda list_out: list_out[0]\n",
    "list_extract = Node(Function(input_names=[\"list_out\"],\n",
    "                             output_names=[\"out_file\"],\n",
    "                             function=extract_func),\n",
    "                    name=\"list_extract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new workflow ``susanflow`` that contains the ``susan`` workflow as a sub-node. To be sure, let's also recreate the ``skullstrip`` and the ``mask`` node from the examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate workflow with name and base directory\n",
    "wf2 = Workflow(name=\"susanflow\", base_dir=\"/output/working_dir\")\n",
    "\n",
    "# Create new skullstrip and mask nodes\n",
    "skullstrip2 = Node(fsl.BET(in_file=in_file, mask=True), name=\"skullstrip\")\n",
    "mask2 = Node(fsl.ApplyMask(), name=\"mask\")\n",
    "\n",
    "# Connect the nodes to each other and to the susan workflow\n",
    "wf2.connect([(skullstrip2, mask2, [(\"mask_file\", \"mask_file\")]),\n",
    "             (skullstrip2, susan, [(\"mask_file\", \"inputnode.mask_file\")]),\n",
    "             (susan, list_extract, [(\"outputnode.smoothed_files\",\n",
    "                                     \"list_out\")]),\n",
    "             (list_extract, mask2, [(\"out_file\", \"in_file\")])\n",
    "             ])\n",
    "\n",
    "# Specify the remaining input variables for the susan workflow\n",
    "susan.inputs.inputnode.in_files = abspath(\n",
    "    \"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\")\n",
    "susan.inputs.inputnode.fwhm = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what this new processing graph looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf2.write_graph(dotfilename='/output/working_dir/full_susanflow.dot', graph2use='colored')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/output/working_dir/full_susanflow.dot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how there is a nested smoothing workflow (blue) in the place of our previous `smooth` node. This provides a very detailed view, but what if you just wanted to give a higher-level summary of the processing steps? After all, that is the purpose of encapsulating smaller streams in a nested workflow. That, fortunately, is an option when writing out the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf2.write_graph(dotfilename='/output/working_dir/full_susanflow_toplevel.dot', graph2use='orig')\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/output/working_dir/full_susanflow_toplevel.dot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much more managable. Now let's execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, let's look at the input and the output. It's exactly what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 4))\n",
    "for i, e in enumerate([[\"/data/ds000114/sub-02/ses-test/anat/sub-02_ses-test_T1w.nii.gz\", 'input'],\n",
    "                       [\"/output/working_dir//susanflow/mask/sub-02_ses-test_T1w_smooth_masked.nii.gz\", \n",
    "                        'output']]):\n",
    "    f.add_subplot(1, 2, i + 1)\n",
    "    plot_slice(e[0])\n",
    "    plt.title(e[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, why are workflows so great?\n",
    "\n",
    "So far, we've seen that you can build up rather complex analysis workflows. But at the moment, it's not been made clear why this is worth the extra trouble from writing a simple procedural script. To demonstrate the first added benefit of the Nipype, let's just rerun the ``susanflow`` workflow from above and measure the execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "wf2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That happened quickly! **Workflows (actually this is handled by the Node code) are smart, and know if their inputs have changed from the last time they are run. If they have not, they don't recompute; they just turn around and pass out the resulting files from the previous run.** This is done on a node-by-node basis, also.\n",
    "\n",
    "Let's go back to the first workflow example. What happened if we just tweak one thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.inputs.smooth.fwhm = 1\n",
    "wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing an input value of the ``smooth`` node, this node will be re-executed. This triggers a cascade such that any file depending on the ``smooth`` node (in this case, the ``mask`` node, also recompute). However, the ``skullstrip`` node hasn't changed since the first time it ran, so it just coughed up its original files.\n",
    "\n",
    "That's one of the main benefit of using Workflows: **efficient recomputing**. \n",
    "\n",
    "Another benefits of Workflows is parallel execution, which is covered under [Plugins and Distributed Computing](./basic_plugins.ipynb). With Nipype it is very easy to up a workflow to an extremely parallel cluster computing environment.\n",
    "\n",
    "In this case, that just means that the `skullstrip` and `smooth` Nodes execute together, but when you scale up to Workflows with many subjects and many runs per subject, each can run together, such that (in the case of unlimited computing resources), you could process 50 subjects with 10 runs of functional data in essentially the time it would take to process a single run.\n",
    "\n",
    "To emphasize the contribution of Nipype here, you can write and test your workflow on one subject computing on your local CPU, where it is easier to debug. Then, with the change of a single function parameter, you can scale your processing up to a 1000+ node SGE cluster."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
